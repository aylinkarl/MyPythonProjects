{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_data_set_ML_practise.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4O6HJk5k8IH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the data set.\n",
        "# 'with_info=True' to get information about version, features and number of instances.\n",
        "# 'as_supervised=True' to load the dataset in a 2-tuple structure (input, target) \n",
        "# 'as_supervised=False' would return a dictionary\n",
        "\n",
        "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info= True, as_supervised= True)"
      ],
      "metadata": {
        "id": "1Yvl_cgDlBbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_info"
      ],
      "metadata": {
        "id": "Tf8h8wgulCCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and testing datasets can be easily extracted.\n",
        "\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# I will use 10% of our training dataset as validation.\n",
        "# 'tf.cast' casts a tensor to a new type.\n",
        "\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "num_test_samples = 0.1 * mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "\n",
        "# Since the possible values for the inputs are between 0 and 255, \n",
        "# we can scale them between 0 and 1 by dividing each element by 255.\n",
        "\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "Jb8vL_wBlCAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'map' allows us to apply a custom transformation to a given dataset\n",
        "\n",
        "scaled_train_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Let's scale the test data to have the same size as train and validation.\n",
        "# No need to shuffle as we won't be training with it\n",
        "# A single batch equal to the size of the test data will suffice.\n",
        "\n",
        "test_data = mnist_test.map(scale)"
      ],
      "metadata": {
        "id": "gEPctstzlB9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will define a buffer size to save memory, this is mostly useful while working big data.\n",
        "# with this TF only stores BUFFER_SIZE samples in memory at a time and shuffles them.\n",
        "# If BUFFER_SIZE=1, there will actually be no shuffling, and of course if BUFFER_SIZE >= number of samples, mixing would be uniform.\n",
        "buf_size = 10000\n",
        "\n",
        "# I will easily use the shuffle method.\n",
        "shuffled_train_validation_data = scaled_train_validation_data.shuffle(buf_size)\n",
        "\n",
        "# I create a batch with a batch size equal to the total number of validation samples\n",
        "# And the rest would be the train data.\n",
        "\n",
        "validation_data = shuffled_train_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_validation_data.skip(num_validation_samples)\n",
        "\n",
        "# Let's determine a batch size.\n",
        "batch_size = 50\n",
        "\n",
        "# batching the train data would be very helpful when we train, as we would be able to iterate over the different batches.\n",
        "train_data = train_data.batch(batch_size)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "# 'as_supervized=True' takes the next batch as we have a 2-tuple structure.\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "metadata": {
        "id": "QNioow83lB7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "# I will use same hidden layer size for all hidden layers\n",
        "hidden_layer_size = 150\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"tanh\"),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"tanh\"),\n",
        "                             tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
        "                             ])"
      ],
      "metadata": {
        "id": "1B8lH5XTmJLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will define the optimizer, the loss function, and the metrics to get at each iteration.\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Rbv7LX4c98wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of epochs\n",
        "max_epochs = 10\n",
        "\n",
        "# Let's set up an early stop mechanism with patience=2 so our model will be somewhat tolerant of random validation loss increase\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "# The accuracy shows in what % of the cases the outputs were equal to the targets.\n",
        "# The val_accuracy shows the true accuracy of the model.\n",
        "model.fit(train_data, epochs= max_epochs, callbacks=[early_stopping], validation_data=(validation_inputs,validation_targets), verbose=2 )\n",
        "\n"
      ],
      "metadata": {
        "id": "eecrG14MlB4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I hope that you find it useful. As I will be happy to improve myself, your comments and feedbacks are always welcome, as are suggestions for additional information that could usefully be included. Thank you! ðŸŒ¸ðŸ˜Š"
      ],
      "metadata": {
        "id": "J9O_7XFMlPGf"
      }
    }
  ]
}